[
  {
    "source": {
      "id": "the-verge",
      "name": "The Verge"
    },
    "author": "Antonio G. Di Benedetto",
    "title": "Lenovo’s flagship gaming laptop has a 2D / 3D screen and carbon fiber lid",
    "description": "Lenovo is announcing a new 10th-generation Legion 9i gaming laptop coming soon to China and scheduled for the fall in North America. The 18-inch behemoth is packed with top-end specs, including the option for a 4K-capable 2D / 3D screen, Nvidia RTX 5090 GPU, …",
    "url": "https://www.theverge.com/news/663084/lenovo-legion-9i-18-rtx-5090-2d-3d-gaming-laptop-announcement-specs",
    "urlToImage": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/05/07_Lenovo_Legion_9i_Eclipse_Black_Hero_Rear_white.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "publishedAt": "2025-05-08T09:38:37Z",
    "content": "The new Legion 9i doesnt have a price yet, but considering the specs and fancy lid, its likely to be high.\r\nThe new Legion 9i doesnt have a price yet, but considering the specs and fancy lid, its lik… [+2560 chars]",
    "full_content": "is a reviewer covering laptops and the occasional gadget. He spent over 15 years in the photography industry before joining The Verge as a deals writer in 2021.\n\nLenovo is announcing a new 10th-generation Legion 9i gaming laptop coming soon to China and scheduled for the fall in North America. The 18-inch behemoth is packed with top-end specs, including the option for a 4K-capable 2D / 3D screen, Nvidia RTX 5090 GPU, Intel Core Ultra 9 275HX CPU, up to 192GB of RAM, and 8TB of SSD storage.\n\nHow much will an over-the-top configuration like that cost? Nobody can say, as Lenovo isn’t talking pricing at all yet — not even the starting price for the base model with a 4K, non-3D screen. But when the time comes you won’t have to rely solely on Lenovo to build the configuration to your liking, as the four RAM slots and four SSD slots are user-accessible.\n\nThe Legion 9i has plenty of RGB lighting, including colorful per-key illumination and a front-facing light bar. Image: Lenovo\n\nThe optional 2D / 3D LCD can display 3840 x 2400 resolution at up to 240Hz in 2D mode or 1920 x 1200 in 3D mode. The screen also has a dual mode akin to the Razer Blade 18, allowing up to an ultrafast 440Hz when displaying 2D content in 1920 x 1080. Viewing 3D content on the Legion 9i’s screen doesn’t require glasses, as it uses eye-tracking and a lenticular lens array to appear 3D to the naked eye. Lenovo says the Legion 9i is designed for game developers, visual artists, and 3D professionals as well as spendy gamers. In addition to displaying 3D content and renders, Lenovo says its software supports 30 games playable in 3D, including Cyberpunk 2077, Black Myth: Wukong, Forza Horizon 5, Genshin Impact, and God of War Ragnarök.\n\nAs you’d expect from a desktop replacement-class laptop, the Legion 9i has a bunch of ports, including two Thunderbolt 5 ports, four USB 3.2 Gen 2 (three Type-A and one Type-C), an ethernet port, HDMI 2.1, and even a full-size SD card slot. For connectivity, it’s got Wi-Fi 7 and Bluetooth 5.4 support. If you go for the 2D screen the 5-megapixel webcam has an f/2 aperture, though the 3D screen option comes with a faster f/1.6 lens — and both webcams feature a kill switch on the side of the laptop for privacy.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n1 / 6 If only the whole laptop could be decked out in this carbon fiber. Image: Lenovo\n\nThe laptop itself has a pretty unique look thanks to its carbon fiber lid. Lenovo says each Legion 9i’s lid is made from eight layers of carbon fiber, making it “lighter and stronger than aluminum.” But any weight savings aside, the coolest aspect of this carbon fiber creation is the visible pattern on the lid — which Lenovo says is slightly different and one-of-a-kind on each unit.\n\nWe may not know how much the Legion 9i costs just yet, but that lid instantly makes it one of the most interesting jumbo-sized gaming laptops in my book. At least, as far as ones not adorned with dragons go."
  },
  {
    "source": {
      "id": "the-verge",
      "name": "The Verge"
    },
    "author": "Antonio G. Di Benedetto",
    "title": "Here’s Alienware’s fresh take on entry-level gaming laptops",
    "description": "Alienware is introducing a pair of new, more affordable gaming laptops: the Aurora 16 and 16X, starting at $1,149 and $1,949, respectively. Unlike the flagship Area-51 laptops announced back at CES, the Auroras are meant to be a little more versatile, portabl…",
    "url": "https://www.theverge.com/news/662857/alienware-aurora-16-16x-gaming-laptop-nvidia-rtx-price-specs",
    "urlToImage": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/05/aw-16-aurora-ac16250-gaming-laptop-awproheadset-awpromouse-front-1.png?quality=90&strip=all&crop=0%2C10.750087716126%2C100%2C78.499824567748&w=1200",
    "publishedAt": "2025-05-07T22:09:18Z",
    "content": "The Aurora 16 and 16X have up to RTX 5070 GPUs, but start with an RTX 3050 and just 8GB of RAM.\r\nThe Aurora 16 and 16X have up to RTX 5070 GPUs, but start with an RTX 3050 and just 8GB of RAM.\r\nAlien… [+3023 chars]",
    "full_content": "is a reviewer covering laptops and the occasional gadget. He spent over 15 years in the photography industry before joining The Verge as a deals writer in 2021.\n\nAlienware is introducing a pair of new, more affordable gaming laptops: the Aurora 16 and 16X, starting at $1,149 and $1,949, respectively. Unlike the flagship Area-51 laptops announced back at CES, the Auroras are meant to be a little more versatile, portable, and comfortable for general laptop use. They replace the older Alienware M, X, and cheaper Dell G lines — which will be slowly phased out as part of Dell’s streamlining of its products.\n\nSome configurations of the Aurora 16 and 16X are launching today in North America, with additional models arriving later.\n\nThe duo of indigo-colored 16-inch laptops are lighter, smaller, and cheaper than the 16- and 18-inch Area-51 models. They’re still pretty hefty at around 5.5 to 5.86 pounds, but much lighter compared to the 7.6 to 10 pounds of the Area-51. And the new models even have a stealth mode button that switches the RGB lighting to a simpler white light and tones down the fans. The Auroras get their name from one of Alienware’s desktop lines, but they’re meant to be taken places, as the 16 and 16X were designed to fit in a backpack thanks to a lack of thermal shelf (the big butt behind the hinge many gaming laptops have). Instead, the Auroras have a protruding bump beneath their chassis where their fans pull in air.\n\nThe Aurora pair look slightly low-key for gaming laptops, aside from their Alien head logos and full number pads. Image: Dell\n\nSpec-wise, both laptops have 16-inch displays, but the Aurora 16 has a 300-nit 120Hz 2560 x 1600 IPS panel, while the 16X has the same resolution but gets up to 500 nits and 240Hz refresh rate. The Auroras have two USB-A, two USB-C, HDMI 2.1, Ethernet, a 3.5mm combo audio jack, a proprietary power plug, and Wi-Fi 7. Each has its ports on the left and rear, freeing up the right side for unhindered mouse movements while gaming. One of the USB-C ports on the 16X gets faster Thunderbolt 4 and DisplayPort 2.1 instead of just USB 3.2 Gen 2.\n\nBoth laptops can be outfitted with up to an Nvidia RTX 5070 GPU, but the Aurora 16 is the entry-level option — with a base configuration in North America equipped with a last-gen RTX 4050. In some countries, such as Brazil and India, the base Aurora 16 comes with a two-generations-old RTX 3050. The pricier Aurora 16X starts with a newer and more powerful RTX 5060.\n\n\n\nThe Aurora 16 is also a step behind on CPUs. Its Intel “Raptor Lake Refresh” chips are based on architecture from 2022, while the 16X has the latest Intel “Arrow Lake” configurations up to the Core Ultra 9 275HX.\n\nImage: Dell Image: Dell Image: Dell Image: Dell\n\nDell-owned Alienware is angling these new models at gamers on a budget or those who can only justify one device for both play and work (or school). I just can’t help finding it funny that, between Alienware and its parent company Dell, it’s actually Alienware with the clearer and more pleasant-sounding naming scheme. Somehow, the try-hard gaming brand with an alien head logo understands that names are better than a mishmash of Plus, Premium, Pro, and Max."
  },
  {
    "source": {
      "id": "the-verge",
      "name": "The Verge"
    },
    "author": "Wes Davis, Lauren Feiner",
    "title": "Apple’s Eddy Cue: ‘You may not need an iPhone 10 years from now’",
    "description": "Eddy Cue, Apple’s senior vice president of services, gave an ominous warning today that the iPhone could go the way of the iPod 10 years from now. And the reason, as one might guess, is artificial intelligence. Cue’s remarks came during the Google Search anti…",
    "url": "https://www.theverge.com/news/662769/apple-iphone-may-not-need-10-years",
    "urlToImage": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/05/gettyimages-2160777605.jpg?quality=90&strip=all&crop=0%2C0%2C100%2C74.160763781953&w=1200",
    "publishedAt": "2025-05-07T17:37:18Z",
    "content": "Cue suggests AI could make the iPhone go the way of Apples former golden goose, the iPod.\r\nCue suggests AI could make the iPhone go the way of Apples former golden goose, the iPod.\r\nEddy Cue, Apples … [+1604 chars]",
    "full_content": "Eddy Cue, Apple’s senior vice president of services, gave an ominous warning today that the iPhone could go the way of the iPod 10 years from now. And the reason, as one might guess, is artificial intelligence.\n\nCue’s remarks came during the Google Search antitrust remedies trial today while discussing how AI has the potential to reshape the tech industry and open the door to new entrants.\n\nIncumbents have a hard time ... we’re not an oil company, we’re not toothpaste — these are things that are going to last forever ... you may not need an iPhone 10 years from now.\n\nCue went on to say that the best thing Apple did was kill the iPod, a move he said was bold. “Why would you kill the golden goose,” he added.\n\nThat may seem like a silly thing for Apple to say, given that more than half of its revenue is iPhone sales. But Cue calls AI a “huge technological shift,” and suggests that such shifts can humble companies that once seemed unassailable. “When I got to Silicon Valley,” he said, “all the best companies or the most successful companies” — he mentioned HP, Sun Microsystems, and Intel — “either don’t exist today or are significantly smaller and much less impactful.”"
  },
  {
    "source": {
      "id": null,
      "name": "Gizmodo.com"
    },
    "author": "Kyle Barr",
    "title": "Microsoft’s New Purple Surface Pro 12 Is Light Enough to Actually Use as a Tablet",
    "description": "The tablet's screen is slightly smaller, but the whole device is also lighter than ever before.",
    "url": "https://gizmodo.com/microsofts-new-purple-surface-pro-12-is-light-enough-to-actually-use-as-a-tablet-2000598245",
    "urlToImage": "https://gizmodo.com/app/uploads/2025/05/Surface-Pro-12-inch-Violet-Hero-1.jpg",
    "publishedAt": "2025-05-06T05:42:51Z",
    "content": "A gadgets price for its size matters now more than ever. It seems more companiesincluding Microsoftare starting to identify with the need for more capable laptops at prices people can actually afford… [+4404 chars]",
    "full_content": "A gadget’s price for its size matters now more than ever. It seems more companies—including Microsoft—are starting to identify with the need for more capable laptops at prices people can actually afford. The Windows maker just dropped details on its new 12-inch Surface Pro and thinner 13-inch Surface Laptop that seem to fit the bill, at least on paper. With prices starting at under $1,000 (not including the Surface Pro’s Flex Keyboard), Microsoft’s own brand of computers could hit the sweet spot in the Venn diagram on looks, price, and performance.\n\nSee Surface Pro at Amazon\n\nSee Surface Pro at Best Buy\n\nThe 2025 Surface family includes two very familiar devices. The $900 Surface Laptop with its 13-inch touchscreen (1,920 x 1,280) has a traditional clamshell design, while the $800 Surface Pro 12 is Microsoft’s headline-grabbing Windows tablet that converts into a laptop-like device thanks to a detachable keyboard. The new Surface Pro is $200 cheaper than last year’s $1,000 11th-gen Surface Pro, but—and here comes the eye roll—you still have to buy a keyboard separately. The screen is also smaller at 12 inches (2,196 x 1,464) compared to the 13-inch display on last year’s tablet. The more compact tablet does have a perk: it weighs 1.5 pounds compared to the Pro 11’s two pounds.\n\nBoth the new Surface Pro and Surface Laptop 13 run on the Snapdragon X Plus chip, Qualcomm’s eight-core, ARM-based CPU variant introduced in the middle of 2024. Last year’s Surface Pro renditions, alongside the 2024 Surface Laptop, included up to the Snapdragon X Elite chip, which is still Qualcomm’s most powerful ARM-based processor. Otherwise, both the new Surface Pro and Surface Laptop come packed with 16GB of RAM and 256GB of storage at base.\n\nMicrosoft says this CPU is supposed to offer a better starting price point and an upgrade path for those on earlier devices like the 2022 Surface Laptop. According to the company, the Snapdragon X Plus chip in the new Surface Laptop is 30% faster than the Intel Core i5-1235U in the 2022 Surface Laptop. Microsoft further claims this CPU can beat the M3 MacBook Air in some benchmarks, but it stopped short of making the same claims about the $1,000 MacBook Air M4. It’s more evidence these big tech firms are willing to start competing on pricing that more people can entertain.\n\nThe new Surface Pro and Surface Laptop aren’t launching with any x86-based Intel chips, though they could arrive later, like with last year’s Surface Pro, which received an Intel Lunar Lake version earlier this year. Either way, Microsoft is sticking with Qualcomm’s ARM chips that offer better battery life on average, though you’ll eventually hit a wall with software compatibility, especially with games or even aging drivers.\n\nThe 12-inch Surface Pro will look very familiar if you’ve used any older version of the hybrid tablet-laptop design. It still includes a built-in kickstand that opens up to sit almost flat on a surface. Other than the shrunken size, the big change this go-around is that the Slim Pen no longer slots into the keyboard (again, sold separately). Instead, the Surface Pro tablet includes an indent in the back for you to slot the stylus. It makes sense, especially for creators who want to use the Surface as a Windows tablet more than a pint-sized PC.\n\nHowever, you may need to find some dry earth to dig a shallow grave to bury your hoard of Surface Connect power cables. The other big change is the loss of the age-old Surface Connect port. Now, the Surface Pro relies only on USB-C with support for 45W fast charging. The Surface Pro’s battery should last around 12 hours based on Microsoft’s tests with active web browsing. The Surface Laptop should get closer to 16 hours before requiring a charge.\n\nJust like last year’s Surface Pro, the new model paints a pretty picture. The brilliantly blue sapphire color of the 2024 Surface Pro is replaced with a violet color that seems spot-on for spring. Both new Surface Pro and Surface Laptop devices also come in platinum or “ocean” colors. The latter is a blue-ish silver that reminds me way too much of Apple’s “Sky Blue” on the MacBook Air M4.\n\nThe changes made to the Surface lineup are minimal, but there are some modifications that make sense. If you loathe the idea of losing Surface Connect, Microsoft confirmed it will continue selling last year’s Surface models, at least for the near future. The next big question is whether these prices will hold with the overriding threat of Trump tariffs. Microsoft reps told us that “tariffs are a moving target for us” and declined to say more about whether we could see Surface price hikes in the near future.\n\nSee Surface Pro at Amazon\n\nSee Surface Pro at Best Buy"
  },
  {
    "source": {
      "id": "the-verge",
      "name": "The Verge"
    },
    "author": "Wes Davis",
    "title": "Apple may release a ‘mostly glass, curved iPhone’ in 2027",
    "description": "This morning, while summarizing an Apple “product blitz” he expects for 2027, Bloomberg’s Mark Gurman writes in his Power On newsletter that Apple is planning a “mostly glass, curved iPhone” with no display cutouts for that year, which happens to be the iPhon…",
    "url": "https://www.theverge.com/news/664776/apple-curved-glass-iphone-2027",
    "urlToImage": "https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/25612700/DSC_3108_Enhanced_NR.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "publishedAt": "2025-05-11T20:03:55Z",
    "content": "The iPhone 16 Pro Max, pictured here, is already pretty glassy.\r\n\n\nThis morning, while summarizing an Apple “product blitz” he expects for 2027, Bloomberg’s Mark Gurman writes in his Power On newslet… [+2249 chars]",
    "full_content": "is a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020.\n\nThis morning, while summarizing an Apple “product blitz” he expects for 2027, Bloomberg’s Mark Gurman writes in his Power On newsletter that Apple is planning a “mostly glass, curved iPhone” with no display cutouts for that year, which happens to be the iPhone’s 20th anniversary.\n\nThat follows a report last weekend from The Information, which said that “at least one 2027 iPhone model that will place the front-facing camera underneath the screen to enable a truly edge-to-edge display.” Late last year, a report from The Elec said Apple is working with its display partners to create a bezel-less iPhone, but not one that curves the display down the side of the phone — a trick companies like Samsung and Vivo have employed in the past.\n\nBut the “mostly glass, curved” part of Gurman’s prediction is more interesting to me, because what the heck does that mean? After all, I’d describe the iPhone 15 Pro sitting on my desk right now as “mostly glass,” with the only exterior metal being around the camera lenses and in its titanium edge, which the front and back curve down to. Assuming he’s not describing a banana-shaped iPhone, the closest hints are probably in Apple patents revealed over the years, like one from 2019 that describes a phone encased in glass that “forms a continuous loop” around the device.\n\nApart from a changing iPhone, Gurman describes what sounds like a big year for Apple. He reiterates past reports that the first foldable iPhone should be out by 2027, and that the company’s first smart glasses competitor to Meta Ray-Bans will be along that year. So will those rumored camera-equipped AirPods and Apple Watches, he says.\n\nGurman also suggests that Apple’s home robot — a tabletop robot that features “an AI assistant with its own personality” — will come in 2027. On that “personality” mention, it’s hard not to think about the adorable robotic lamp Apple’s internal researchers have been tinkering with."
  },
  {
    "source": {
      "id": null,
      "name": "Hardenedvault.net"
    },
    "author": "HardenedVault",
    "title": "Why Intel Deprecated SGX?",
    "description": "The rumors about Intel SGX deprecated in new processors has been confirmed, 12th generation processors (Workstation/Desktop/Laptop/embedded platforms) will deprecate SGX and the SGX will continue to support only in high-end Xeon CPU for server:",
    "url": "https://hardenedvault.net/blog/2022-01-15-sgx-deprecated/",
    "urlToImage": "https://hardenedvault.net/images/blog/sgx-dep.jpg",
    "publishedAt": "2025-05-08T05:32:18Z",
    "content": "The rumors about Intel SGX deprecated in new processors has been confirmed, 12th generation processors (Workstation/Desktop/Laptop/embedded platforms) will deprecate SGX and the SGX will continue to … [+7697 chars]",
    "full_content": "January 15, 2022 | 6 min Read\n\nThe rumors about Intel SGX deprecated in new processors has been confirmed, 12th generation processors (Workstation/Desktop/Laptop/embedded platforms) will deprecate SGX and the SGX will continue to support only in high-end Xeon CPU for server:\n\nIntel’s official explanation is that final decision is made due to market reasons. Intel SGX (software guard extension) has been shipped with the release of the 6th generation processor Skylake in 2015. Its main purpose is to better solve the trust issue in cloud environments between CSPs (cloud service provider) and tenants. SGX proposes a solution called enclave where OS isn’t able to access to. The technical details of SGX has a lot of controversy since the beginning. This article will explore a bit about SGX:\n\nRoot problem: complexity\n\nIntel SGX is the most sophisticated implementation among the enclave solutions. Intel did a lot of work at multiple levels to leverage the support:\n\nHardware, e.g: MEE\n\nUcode, for special instructions\n\nFirmware, Intel CSME infrastructure upgrade and multiple CSME modules getting involved\n\nSGX basics\n\nSGX hands over paging (EPC) to an untrusted OS, which is similar to BASTION, where the host OS can evict.\n\nSGX uses Intel EPID to implement attestation, which is too complex for microcode to implement.\n\nIn addition to EPID, SGX also uses other CSME code modules such as iclsClient using CLS (Capability Licensing Services)\n\nAttestation process\n\nSeal Secret and Prospering Secret are stored in e-fuses, Provisioning Secret is generated by Intel Key Generation Facility and burned to the CPU and saved in Intel xx service, Seal secret is generated inside the CPU, theoretically unknowable to Intel.\n\nEGETKEY uses certificate-based identify (MRSIGNER, ISVPRODID, ISVSVN) and SGX implementation version (CPUSVN) to get the Provisioning key, which allows the Intel provisioning service to verify that Provisioning Enclave is signed by Intel. The provisioning service can also refuse communication based on CPUSVN to determine if there is a vulnerability.\n\nWhen Provisioning Enclave obtains the Provisioning Key and communicates with the Intel Provisioning service and authenticates itself, the service generates an Attestation key to provision the Enclave, which encrypts the AK using the Prospering Seal key and saves it.\n\nAK uses the EPID cryptography, EPID as a group signature scheme to provide some anonymity for the signer, Intel key provisioning service is the issuer, it will publish the Group Public Key and will save the Master Issuing Key itself, after the provisioning enclave verifies itself to the service, The service generates an EPID member private key as an AK and then executes the EPID Join protocol to join the signature group, after which Quoting Enclave uses the EPID MPK to generate an attestation signature.\n\nRisk： Issued SIGSTRUCT was leaked, and attackers could use the SGX debugging feature to build debugging provisioning or Quoting enclave to modify the code, or get a 128-bit provisioning key to communicate with the Intel service.\n\nAccording to Intel’s patent, the implementation of SGX relies on the complex KDF process inside the CPU circuit for the global secret keys and stored in the eFUSE, Chipworks offers $50-250k to fully extract the eFUSE of one Intel i5 processor, so the eFUSE content is encrypted by a master key (called “global wrapping logic key” in the patent). GWK is used to encrypt a 256-bit message for regenerating the EPID key of the CPU and a handful of 128-bit pre-seed key 0, eFUSE also contains a plaintext copy of 128-bit pre-seed key 1 and a 32-bit EPID group ID, GWK is hard-coded into the chip circuit, all chip manufacturers share the mask set, such a process increases the cost of the attack, But there is also the possibility of being reversed.\n\nSGX also uses PUF to generate symmetric keys for the device during the provisioning phase, the PUF key is encrypted by GWK and transmitted to the key generation server, after which the key generation server encrypts the fuse key of the chip with the PUF key and then transmits it to the chip, the PUF key increases the cost of obtaining the chip fuse key. The attacker must compromise provisioning stage simultaneously.\n\nCSME also has an eFUSE for saving the EPID key for fTPM. The first scheme is that Provision enclave uses the provisioning seal key to encrypt the DAK, which assumes that CSME is an untrusted flash memeory, so fTPM cannot be used. Another option is to use the key agreement protocol to establish a secure communication channel between DMI buses, which ME fw can be used to store DAKs or to implement fTPM.\n\nWrong threat model in the very beginning\n\nIntel SGX puts the owner of on-premise (cloud service vendors, system administrators, etc) into the threat model in the 1st place. Technically, SGX doesn’t trust the operating system and the entire stacks of firmware (except CSME), but Intel might have missed an important common sense: OS kernel may no longer the “CORE” but it’s still the entrance to the “underworld”, as the 0ldsk00l hacker was joking about “don’t make jokes about the underworld”. Most of the attacks targeting SGX are based on having kernel privilege. A proper threat model won’t guarantee a comprehensive solution for security but it could be the starting point at least.\n\nWrong threat model isn’t the only issue in SGX：\n\nOver-design and implementation leads to out-of-control complexity.\n\nLack of transparency, Intel SGX implementation is closed-source in terms of dependencies on the underlying firmware, which means it can’t be audited properly with acceptable cost. While SGX is highly dependent on Intel CSME, yet another issue about “god” mode in CSME can be referred to HardenedVault’s Intel CSME Risk Assessment.\n\nSGX can be used to protect malware, which malware detection become an impossible mission.\n\nIntel didn’t open up SGX-based third-party attestation services to SME client until December 2018, a decision that may have been a little late from a market and ecological perspective.\n\nSGX’s Linux kernel mainline process is slow. in April 2016, Intel submitted the first version of the SGX patch to the Linux kernel community, the Linux kernel community believes that there are many unresolved basic issues, including ABI compatibility issues and SGX as the core of enclave computing assumptions: if the Linux kernel is compromised, SGX can guarantee that the application is not interfered with by attackers. Even if this premise is correct, the kernel developer’s question is: If there is a malicious enclave application, who will protect the kernel? Moreover, the first premise has been denied by the industry after L1TF was exposed (although there were also relevant studies exposed before but the media did not report on a large scale), the exposure of L1TF and cryptocurrency bubble crashed in 2018 broke many people’s silver bullet expectations for SGX. A series of issues delayed the upstreaming until Linux kernel v5.11 merged SGX into mainline in Feb 2021.\n\nLaunch low cost of side-channel attacks with Linux kernel privileges.\n\nOver-hype in the market, this problem may be more prominent in China. The megacorp continues to advocate that SGX can become the “next generation” silver bullet level program, but the reality is that the general principle in infosec is that there is no silver bullet.\n\nIs SGX still an effective security feature?\n\nYes, SGX is still an effective security mechanism to protect your digital assets. Intel has adjusted its expectation for SGX and it’s targeting server-only markets. From the perspective of the production environment, SGX is still a very effective security mechanism that can be utilized to build your own defense-in-depth “Cyber Bunker”."
  },
  {
    "source": {
      "id": null,
      "name": "Abortretry.fail"
    },
    "author": "Bradford Morgan White",
    "title": "Intel: Winning and Losing",
    "description": "Idling at a Zenith",
    "url": "https://www.abortretry.fail/p/intel-winning-and-losing",
    "urlToImage": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b12431-fd71-4ca8-8731-9dabdaeff7f9_1100x450.webp",
    "publishedAt": "2025-05-10T11:19:39Z",
    "content": "This article continues a lengthy series. You may be interested in the start of silicon valley, Fairchild, the founding of Intel, the start of the x86 architecture, Intels pivot to become a processor … [+26927 chars]",
    "full_content": "This article continues a lengthy series. You may be interested in the start of silicon valley, Fairchild, the founding of Intel, the start of the x86 architecture, Intel’s pivot to become a processor company, the i960 and i486, the Intel Inside campaign, the FDIV bug and the Pentium Pro, MMX, the Pentium II, and the Pentium III, Pentium M, and the launch of Intel Core.\n\nA quick note, up to this point, I have avoided using Intel’s microarchitecture names as the company’s overall product branding and node size were sufficient. From this point forward, I will have to start using Intel’s naming to differentiate products as their product branding became… excessively numeric. Also, to put things into historic perspective as previously mentioned, the Pentium would have been the i586, the Pentium Pro (also Pentium M) would have been the i686, the Pentium 4 would have been the i786, the 64bit Intel Core family would have been the i886.\n\nAt the start of 2008, Intel was in a strong position. While they no longer had the handheld space, they were the kings of both the desktop and laptop CPU market. No one could rival the performance of their hardware, and no one could rival their manufacturing capabilities. The company did, however, have a gap in their offerings.\n\nIntel Atom Bonnell die shot, image from Intel\n\nOn the 2nd of March in 2008, Intel introduced the Atom CPU family with the Bonnell microarchitecture manufactured on the company’s 45nm High-K Metal Gate process. Here, Intel reintroduced the classic 32bit x86 design. The first Intel Atom CPUs were in-order, most instructions were not translated into RISC micro-operations, and there was no on-die memory controller. Intel did bring hyperthreading back with the Atom, and they also gave it SSE. Interestingly, the Bonnell Atoms lacked dedicated integer multiply/divide and those tasks were handed off to SIMD floating point units. With in-order execution, memory latency was a serious concern so the Atom gained a 32K L1 instruction cache, 24K L1 data cache, and a 512K L2. The Atom also had two hardware prefetchers with one from L2 to L1 and the other pulling from system memory into the L2. These chips were paired with the Intel Poulsbo chipset manufactured on a 130nm process. Poulsbo provided both northbridge and southbridge with 2x PCIe, 8 USB2 host ports, 1 USB2 client port, 3x SDIO/MMC, up to 1GB of DDR2, IDE, Azalia audio, and Intel GMA 500 graphics (licensed Imagination Technologies PowerVR SGX graphics core and VXD HD video engine). This graphics situation is a bit awkward. The chip was capable of decoding a 1080p video, but it could only output 1366x768. It could, however, do that decoding with just 120mW.\n\nThe first Atom parts released were codenamed Silverthorne, and these were single core parts. The least power hungry was the Z500 at 80mW idle (160mW average, 0.65W TDP) with an 800MHz clock and 533MHz FSB. The most powerful was the Z540 at 1.86GHz with a 533MHz FSB consuming 100mW at idle (220mW average, 2.4W TDP). These could be paired with an Intel wireless chip and battery controller and then became Intel Centrino Atom. The second series were the N230, N270, and N330. The first two were launched late June, and the third in September. These increased the L1 cache to 64K, brought the Atom to 64bits, and in the case of the N330, brought the core count to two. While these later Diamondville parts support EM64T, most of the systems they were used in did not. Also, despite these chips being efficient, roughly the size of a grain of rice, and often beating ARM chips of the time in performance, Poulsbo was a physically large package, and this made Atom unsuitable for handheld products. Total system power draw was also typically greater than the range required for a handheld of the time.\n\nQuad core Nehalem die shot, image from Intel\n\nOn the 11th of November in 2008, Intel launched the Nehalem microarchitecture starting with the Bloomfield series of Core i7 and Xeon CPUs. In Intel’s classical naming scheme, these would have been the i986 line. These processors were built for the new LGA1366 socket, were fully 64bit while retaining 32bit and 16bit backwards compatibility, and supported three channel DDR3 RAM with an on-die memory controller. These chips brought SSE4.2, TurboBoost, and VT-x with extended page tables to Intel’s product offerings. With both the PCIe 2 controller, memory controller, and DMI (Direct Media Interface) on-die, Intel effectively abandoned the normal northbridge/southbridge chipset distinctions in use on previous CPUs, and they implmented QPI (QuickPath Interconnect) to link CPUs, memory, and I/O controllers. These new CPUs were also multicore on a single die and this necessitated a few more changes. Caches were now 32K L1 instruction, 32K L1 data and 256K L2 per core with an 8MB L3 shared among all cores. Hyperthreading was reimplemented on Nehalem as SMT (simultaneous multithreading) and allowed two threads to be run through each core at the same time. The first iteration of Nehalem was code-named Bloomfield, and the first Bloomfield CPUs were built on Intel’s 45nm process of 731 million transistors. There were three initial parts in the Bloomfield lineup, and they were all Intel Core i7 SKUs: 920, 940, 965. These all had a TDP of 130W, four cores, eight threads, 8MB of L3, and they differed only in QPI bandwidth and clock speed. The 920 had a base clock of 2.66GHz and turbo of 2.93GHz with 4.8GT/s QPI, the 940 had a base clock of 2.93GHz and turbo of 3.2GHz with 4.8GT/s QPI, and the 965 had a base clock of 3.2GHz and turbo of 3.46GHz with 6.4GT/s QPI. All this new technology, and the Nehalem chips used roughly 20% less power than Core 2 while still delivering on better performance.\n\nToward the end of the year, Craig Barrett announced that he would be resigning as chairman effective in May of 2009 with Jane Shaw taking over that role. The company posted income of $5.29 billion on revenues of $37.5 billion for 2008, and while that was lower than the year prior (unsurprising with the 2008 economic turmoil), the company’s operating income was higher, R&D spend roughly the same, long term debt was roughly $100 million lower, capex was about a billion higher, and the employee count was around 3000 lower.\n\nWithin the first quarter of 2009, Intel released Nehalem based Xeons. Most of these were quad-core parts, but there were a few dual core parts as well. TDPs ranged from 38W to 130W, caches from 4M to 8M, QPI speeds from 4.8GT/s to 6.4GT/s, and base clocks from 1.86GHz to 3.33GHz.\n\nThe company announced its 32nm process in February. This process was a continuation of the High-K Metal Gate used previously, but it added self-aligned via patterning. The lithography was 193nm, the platter was 300mm, gate length was 30nm, and gate pitch was 112.5nm.\n\nAs expected, Jane Shaw became chairman in May. She’d earned her Ph.D in physiology from Birmingham University in England, and had most recently served as chairman of Aerogen. While she’d been a member of the board since 1993, she was the first chairman to be brought in from outside of Intel since Arthur Rock at the company’s founding rather than having been the company’s CEO.\n\nLater in the year, mainstream Nehalem/Bloomfield CPUs became available in single core, dual core, and quad core varieties, and mobile Nehalem/Clarksfield parts became available in quad core varieties.\n\nIntel ended 2009 with income of $4.36 billion in income on revenues of $35.1 billion. The company’s debt went up to $2 billion, but this wasn’t really a threat given that Intel had cash holdings of $11.1 billion. The company’s workforce was reduced to 79,800.\n\nIntel Westmere die photos, image from Anandtech\n\nOn the 7th of January in 2010, Intel announced the Nehalem/Westmere microarchitecture using Intel’s 32nm process. The low-end lineup included dual core parts without hyperthreading for Celeron and Pentium, and dual core parts with hyperthreading for i3 and i5. The i7 parts were now six core twelve thread parts. The socket situation was unfortunate with low-end parts using LGA1156 while high-end parts used LGA1366, and multiprocessor Xeons used LGA1567. Although, Xeon versions of each segment were available with the same sockets as the consumer variants, the multiprocessor parts (as opposed to single processor or dual processor) used their own special socket. Mobile variants of Celeron, Pentium, i3, i5, and i7 were made available later in the year. This generation had the ability to shutdown individual cores to save power, turbo higher on all cores to deliver performance, had AES encryption/decryption acceleration, on-package GPU for dual core parts, support for the virtualization of 16bit real mode, and support for huge pages of 1GB. The basic premise of this last bit is that a CPU allocates RAM to a process in chunks called pages. The default page size is often 4K and when a system has a large amount of memory, the number of pages can get large. The more pages, the higher the performance penalty in locating memory. For a 4K page size, a process using 1GB of RAM would have 262144 entries. A larger page size would reduce the number of page table entries. Larger page sizes are of more relevance to the server market than consumers partly due to servers often having far more memory, but also due to software compatibility. Consumers tend to run a wide variety of software on a single machine while servers typically run one or just a few applications. Not all software will work with large page sizes.\n\nIntel SSD 310, image from Intel\n\nIn December of 2010, the Intel SSD 310 saw the light of day. This was an mSATA SSD that used an mPCIe edge. It was a 34nm MLC NAND flash drive and came in 40GB and 80GB versions. The 40GB drive consumed just 75mW at idle, and the 80GB doubled that to 150mW. For performance, the 40GB drive could offer 170MB/s reads and 35MB/s writes, while the 80GB model could offer 200MB/s reads and 70MB/s writes. All of this with a physical size of just 50.8mm long, 29.85mm wide, and 4.85mm thick. It weighed just 10 grams. Early adopters were both Lenovo (ThinkPads) and DRS Technologies (tablets).\n\nThroughout 2010, the average sales for PCs crossed the 1 million per day mark, the PC market grew by 17% globally, Internet traffic was greater than all previous years combined, and Intel benefited greatly. Even the humble, Intel Atom was popular (particularly in the netbook market), and Intel shipped the 80 millionth Atom in 2010. Intel closed 2010 with $11.4 billion in income on revenues of $43.6 billion. Their employee count was up to 82,500 debt was up to $2 billion, and the company held nearly $16.7 billion in cash.\n\nAt the start of 2011, Intel had come to the realization that their hegemony wasn’t guaranteed, and that it may be under threat. The company identified its competitors in the PC space as being AMD, Qualcomm, and VIA, in the server space the list was AMD, IBM, and Oracle (Sun), and in the embedded space AMD, Broadcom, Freescale, MediaTek, Nvidia, Qualcomm, Samsung, STM, and TI. Where once Intel had ruled even the embedded market with XScale supplying both Blackberry and Palm, the company was now almost entirely absent from that market. To attempt to remain relevant in the low-power markets, Intel began investing heavily in Atom and the ecosystem around it. The company began a Linux project called MeeGo, initiated the Atom Developer Program, announced a partnership with Google around Android, a partnership with Motorola Mobility around phones and tablets, and began working with ZTE on an Atom-based smartphone.\n\nOn the CPU side of things, Intel launched Sandy Bridge on their 32nm process on the 9th of January. These improved upon Nehalem and offered advanced vector extensions, up to eight physical cores per chip, a 1536 entry micro-op cache, larger scheduler buffer, hardware acceleration for video encoding and decoding on units with an integrated GPU, and moved the integrated GPU on-die (as opposed to on-package). Sandy Bridge offered roughly an 11% performance increase over Nehalem at the same clock, and the iGPU was roughly double the performance of the previous generation. Sandy Bridge chips were available in a huge number of SKUs across Celeron, Pentium, i3, i5, i7 and i7 Extreme. These were the Core i3 2100T through Core i7 2700K for socket LGA1155. Both Xeon and Mobile parts were also made. These are considered the second generation Core CPUs, and with the addition of AVX and integration of the GPU, these would have been the i801086 series in Intel’s original naming scheme. So, here we see that Intel marked a break with both Intel Core Duo/Solo and Core 2, and they were considering Nehalem (i986) as their starting point for the current and future generations of Core.\n\nAt the 2009 Intel Developer Forum, the company introduced Light Peak with a prototype Mac Pro running two 1080p video streams, LAN, and storage devices over a single optical cable measuring approximately 30 meters terminated with a variation on USB. This was driven by a PCI Express card with two optical buses, two Light Peak ports per bus, and with each port offering 10Mbit/s. Light Peak was then seen at Intel’s European research showcase in May of 2010 in Brussels achieving much the same performance from a laptop. In January of 2011, Light Peak reached the world as Thunderbolt, and it combined PCI Express, DisplayPort, and DC power into a single port and cable via two serial signals. Thunderbolt can also be daisy chained. The first versions of this standard used the same physical connector as Mini DisplayPort, and utilized copper cabling rather than optical. This change was largely to enable power delivery. The first devices to feature Thunderbolt were Apple MacBooks Pro launched in February, with iMacs following in May, and Macs Mini and MacBooks Air following in July.\n\nOn the 28th of April in 2011, Intel completed the acquisition of McAfee for $7.68 billion. This was Intel’s largest acquisition to date, and the company’s share price dropped by 3.5% on the news. The acquisition made little sense to many at the time, but McAfee had around $1.93 billion in subscription revenue, Intel was trying to diversify, trying build a software ecosystem around their products, and security had been a growing concern in PC market since the advent of the Web.\n\nIntel tri-gate transistor, image from Intel\n\nOn the 4th of May in 2011, Intel announced its 22nm process to the world, and here again, Intel led the way to an entirely new transistor technology. Up to this point, the market had used strictly planar transistors. With Intel’s 22nm process, the world entered the era of the FinFET. In a FinFET design, the gate surrounds the channel on three sides providing a reduction in power consumption and a lowering of propagation delay (the time required for signal to travel through a transistor). This was Intel’s third generation of high-k metal gate transistors, and calling it 22nm was… an odd choice. The fin width was 8nm, the the fin height was 34nm, the fin pitch was 60nm, the gate length was 26nm, and the gate pitch was 90nm. None of those specifications are actually 22nm.\n\nCloser view of the fin, image from Intel\n\nHistorically, a process node’s common designation was derived from the width of the transistor gate and half-pitch (half of the distance between two of the same features on a chip). Thus, if one were discussing a 1 micron process, both the gate width and half-pitch were roughly 1 micron in size. On the prior 32nm process, the gate length was close to the 32nm, but was actually smaller. This was also true of Intel’s 45nm process, where the gate length was actually around 25nm. So, had Intel been changing the rule to naming the process after the gate length rather than width, these node names were indicative of something. Here, the gate length is larger. So, why the name? Well, these naming rules lost all meaning. First, shrinking gate length and width ceased having meaningful positive impacts on performance at around 25nm (or, Intel’s 45nm process). With FinFET, transistor density could continue to increase despite an effective freeze on geometries. Over time, the number of interconnects and metal layers also increased as fabrication technologies improved. This allowed for more logic optimization and increases in performance. So, the name? As far as Intel’s process node names are concerned, one could really think of this as: given the original performance and transistor densities of early CPUs, what would the process node need to be to yield what is now on display?\n\nOn the 31st of May at Computex, Intel launched the Ultrabook initiative replacing Centrino. Apple’s MacBook Air had made waves a few years before, and it made use of Intel’s CPUs. Intel wanted to push similar designs across the industry as laptops and tablets became more common. Intel had begun providing design sheets to OEMs in the 1990s, and this was an evolution of that process. An ultrabook was effectively a thin and light laptop that still packed some performance. More specifically, the laptop in question had to weigh no more than 3.1 pounds, measure no more than 0.71 inches in thickness, offer a minimum of five hours of battery life, and resume from hibernate in a maximum of 7 seconds. The very first ultrabooks were released before the year’s end.\n\nIntel Xeon Phi, Knights Corner die shot, image from Intel\n\nIn June of 2011, SGI announced that they intended to use Intel MICs (many integrated cores) along with Intel Xeon CPUs in their upcoming supercomputer. This Intel MIC product was codenamed Knights Corner and would be built on Intel’s 22nm process. Intel already had an extremely strong presence in the supercomputer market powering about 80%, but the MIC was new. In November, Intel demonstrated the Knights Corner coprocessor capable of 1TFLOPS. This was the first time a single chip could achieve this level of performance. In 1997, this was the performance of the world’s best supercomputer, ASCI Red, and as of 2011 that power could be had in a single chip from Intel. The cores of Knights Corner were based upon the original Pentium design, not unlike the Intel Atom, but added 4-way SMT, 512bit SIMD (similar to AVX-512), 64K L1, 512K L2, and a ring bus connecting processors and memory.\n\nIn December, the company announced it had completed yet another restructuring. The company was now divided into several major segments: PC Client Group (PCCG), Data Center Group (DCG), Mobile Communications, Intelligent Systems, Netbook and Tablet, Ulta-Mobility, McAfee, Wind River, Software and Services, Non-Volatile Memory Solutions. For 2011, PCCG was responsible for roughly two thirds of Intel’s revenues, and DCG was responsible for about 19%. Otherwise stated, high performance CPUs comprised roughly 85% of Intel’s total revenues. Said total revenues for 2011 stood at a mighty $53.9 billion, income was $12.9 billion, cash was $20.9 billion, debt was $7 billion, assets were $71.1 billion, and the company had roughly 100,100 employees.\n\nIntel Medfield phone reference design from Intel\n\nIntel announced the Medfield platform built around the Intel Atom in January of 2012. Atom at this point continued to be dual-issue, in-order, hyperthreaded, had a sixteen-stage integer pipeline, and had no dedicated integer multiply or divide (used the FP unit for that). It had 24K L1 and 512K L2, and an ultra-low-power SRAM of 256K which held CPU state and cache data when the CPU was in its lowest power sleep state. The Atom CPU was mated with the PowerVR SGX 540 which was clocked at 400MHz. For video decode and encode, Medfield made use of the VDX385 and VDE285. Medfield was manufactured on Intel’s 32nm process and consumed 50mW at 100MHz, 175mW at 600MHz, 500mW at 1.3GHz, and 750mW at 1.6GHz. While Intel had hoped to offer its own MeeGo Linux distribution for its mobile ambitions, they knew where the market was and Google announced Android support for Medfield, and it was Intel itself fixing bugs andsubmitting changes to AOSP while trying to make the Intel port of Android a truly good choice. To get to this point, Intel had hired Mike Bell (formerly of Apple and Palm) and given him quite a bit of freedom to assemble his team and get the work done. When the Gigabyte Orange Santa Clara was benchmarked against then current ARM phones running Android in March, it ranked third. Medfield lost to the Xiaomi Mi-One Plus and Asus Transformer Prime, but beat the Samsung Galaxy Nexus.\n\nIntel’s work may not have changed the world, but it did give the company a few wins. In April of 2012, the Lava Xolo X900 first released in India became the first Intel Atom powered smartphone available to consumers. It made use of a Medfield SoC, 1GB of RAM, 16GB of flash storage, had NFC capabilities, a 1024x600 LCD, an 8 megapixel camera, and ran Android 2.3 at launch though Android 4 came quickly thereafter. In August of 2012, ZTE released the Grand X IN on the Medfield platform with Android 4.0. It had a 4.3-inch screen at 960x540. The Lenovo K800 was released in September of 2012 with a 4.5 inch 1280x720 IPS display, 1GB of RAM, 16GB flash, micoSDHC expansion, 8MP camera, 802.11n, GPS, Bluetooth 2.1, and Android Gingerbread. From everything I can find, the Lenovo K800 was the best of the first round of Intel powered smartphones, and was generally well received.\n\nIn April of 2012, Intel’s third generation Core CPUs, Ivy Bridge, made their way to the market on Intel’s 22nm process intended for LGA1155. These once gain spanned the Celeron to Xeon line up. These were mostly a shrink of the Sandy Bridge chips, but thanks to the 22nm process offered nearly a 50% reduction in power consumption. The RDRAND instruction was also added.\n\nIn May, Jane Shaw retired, and Andy Bryant became chairman. In June, Knights Corner became Intel Xeon Phi and came in SKUs of 57, 60, and 61 cores. Xeon Phi did indeed power some of the best supercomputers. The Ultrabook initiative didn’t do quite as well as Intel had hoped. Sales for 2012 were around 10 million. Disappointing ulrabook sales didn’t hurt the company at all though, and the chip giant closed 2012 with $53.3 billion in revenue, $11 billion in income, and 105,000 employees.\n\nFor those watching Intel, 2013 wasn’t very exciting. Honestly, at this point in Intel’s history… even FinFET wasn’t quite as exciting as High-K Metal Gates, and that wasn’t quite as exciting as anything going on in the 1990s. That this article is late probably says more about how I feel about this era than anything else. The biggest stories are the retirement of Paul Otellini, Brian Krzanich’s subsequent promotion to CEO, the Thunderbolt 2 release, and the Haswell release.\n\nKrzanich was born on the 9th of May in 1960 in Santa Clara. He attended San Jose State University and earned a bachelor’s in chemistry. Nearly immediately after graduation, he began working at an Intel fab in New Mexico in 1982. He rose to be manager of a fab in Chandler, Arizona in 1996. He led factory and supply chain management at Intel starting in 2007, and became COO in 2012. His elevation to CEO occurred in May of 2013.\n\nThe launch of Intel’s fourth generation core series of processors occurred on the 4th of June in 2013. Haswell chips continued most of what was seen with recent Intel chips, but supported up to 32GB of dual channel DDR3 on socket 1150, up to 16 lanes of PCIe3, added AVX2, integrated the voltage regulator, added some new sleep states, and brought Direct3D 11.1 and OpenGL 4.3 support to the integrated GPU. Mainstream parts ranged from 2 to 4 cores, enthusiast and high-end desktop stood at 6 or 8 cores, and Xeons could be had with up to 18 cores.\n\nShortly after the launch of Haswell came Thunderbolt 2 offering 20Gbit/s by allowing the two 10Gbit/s channels to be joined. Thunderbolt 2 first debuted on the Mac Pro and was then seen on MacBooks Pro later in the year. The only real differences between versions 1 and 2 are link aggregation, but this has the benefit of allowing 4K video to be run over Thunderbolt using DisplayPort 1.2.\n\nThe ultrabook initiative did better in 2013 with 44 million units sold, but Intel’s ambitions for Atom continued to disappoint. While a few more models of Atom handhelds made it to market, none were widely popular. The best selling phones of 2013 were the Nokia 105, iPhone 5S, Galaxy S4, Galaxy Note 3, and LG G2 all of which were ARM based. Intel closed the year with $9.6 billion in income on revenues of $52.7 billion, and the company consisted of 107,600 people.\n\nIntel’s work on their 14nm process began in Fabruary of 2011 in Chandler, Arizona. This was Fab42, and it was initially scheduled to be operational in 2013, but on the 17th of May in 2011 that timeline was revised to 2014. By 2014, three locations were ready to begin production: Arizona, Oregon, Ireland. Anyone who’d been hoping for a release of parts early in the year would have been sorely disappointed. Intel struggled to get yields up, and on the 11th of August, the company publicly addressed the issue with a presentation titled “Advancing Moore’s Law in 2014.” With this slide deck, we learned more details about the process and about the first consumer products to be produced on that process. Amazingly, Intel reduced the transistor fin pitch from 60nm to 42nm, transistor gate pitch from 90nm to 70nm, and the interconnect pitch from 80nm to 52nm. Further, gate length was now just 20nm, and voltage was down to just 0.7V. Not only was Intel first to 14nm, but the transistor density they achieved was far higher than any other in the industry at a process node by that name.\n\nThe first product made available on 14nm was the Intel Core M. The first generation Intel Core M series was based upon Intel’s fifth generation Core microarchitecture (Broadwell) but was targeted at much the same market as Atom. Four SKUs were launched on the 6th of September in 2014 and these were all dual core hyperthreaded parts with 4MB of L3 and 4.5W TDP. They varied primarily in base clock and turbo with the 5Y10c being 800MHz/2GHz and the 5Y71 being 1.2GHz/2.9GHz. These parts integrated the platform controller hub on package. For Broadwell more generally, Intel’s new chips enjoyed a 5% IPC improvement over Haswell, floating point multiplication was faster, the graphics engine was far better and all SKUs saw an increase in the number of execution units.\n\nIntel continued to make quite a bit of money, and the company closed 2014 with $11.7 billion in income on revenues of $55.8 billion. Intel had $91.9 billion in assets, $13.7 billion in debt, $20 billion in cash, and 106,700 employees. Despite a rocky start, the company’s 14nm process was a generation or more ahead of the competition, and it’s easy to imagine Intel’s leadership believing that things couldn’t possibly change. While Intel had very clearly lost in the smartphone market, they were still dominant in laptops, desktops, workstations, and servers, and they did have networking products serving the cellular markets (like the 7260 LTE modem or the company’s WiFi and Bluetooth chips). First with Atom and now with Core M, they still had a place in tablets. Naturally, when companies feel safe, they usually aren’t…\n\nI have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment."
  },
  {
    "source": {
      "id": null,
      "name": "Hackaday"
    },
    "author": "Jenny List",
    "title": "A Single Chip Computer For The 8051 Generation",
    "description": "The Intel 8051 series of 8-bit microcontrollers is long-discontinued by its original manufacturer, but lives on as a core included in all manner of more recent chips. It’s easy to understand …",
    "url": "https://hackaday.com/2025/05/09/a-single-chip-computer-for-the-8051-generation/",
    "urlToImage": "https://hackaday.com/wp-content/uploads/2025/05/8051-sbc-featured.jpg",
    "publishedAt": "2025-05-09T18:30:17Z",
    "content": "The Intel 8051 series of 8-bit microcontrollers is long-discontinued by its original manufacturer, but lives on as a core included in all manner of more recent chips. It’s easy to understand and prog… [+1102 chars]",
    "full_content": "The Intel 8051 series of 8-bit microcontrollers is long-discontinued by its original manufacturer, but lives on as a core included in all manner of more recent chips. It’s easy to understand and program, so it remains a fixture despite much faster replacements appearing.\n\nIf you can’t find an original 40-pin DIP don’t worry, because [mit41301] has produced a board in a compatible 40-pin format. It’s called the single chip computer not because such a thing is a novelty in 2025, but because it has no need for the support chips which would have come with the original.\n\nThe modern 8051 clone in use is a CH558 or CH559, both chips with far more onboard than the original. The pins are brought out to one side only of the board, because on the original the other side would interface with an external RAM chip. It speaks serial, and can be used through either a USB-to-serial or Bluetooth-to-serial chip. There’s MCS-BASIC for it, so programming should be straightforward.\n\nWe can see the attraction of this board even though we reach for much more accomplished modern CPUs by choice. Several decades ago the original 8051 on Intel dev boards was our university teaching microcontoller, so there remains here a soft spot for it. We certainly see other 8051 designs, as for example this Arduino clone."
  },
  {
    "source": {
      "id": null,
      "name": "Realitystudio.org"
    },
    "author": null,
    "title": "Charles Bukowski, William Burroughs, and the Computer (2009)",
    "description": "Reports from the Bibliographic BunkerJed Birmingham on William S. Burroughs Collecting 16-bit Intel 8088 chip with an Apple Macintosh  ...",
    "url": "https://realitystudio.org/bibliographic-bunker/charles-bukowski-william-burroughs-and-the-computer/",
    "urlToImage": null,
    "publishedAt": "2025-05-10T00:45:47Z",
    "content": "Reports from the Bibliographic Bunker\r\nJed Birmingham on William S. Burroughs Collecting\r\n16-bit Intel 8088 chip\r\nwith an Apple Macintosh you can’t run Radio Shack programs in its disc drive. nor can… [+21417 chars]",
    "full_content": "Reports from the Bibliographic Bunker\n\nJed Birmingham on William S. Burroughs Collecting\n\n16-bit Intel 8088 chip\n\nwith an Apple Macintosh\n\nyou can’t run Radio Shack programs\n\nin its disc drive.\n\nnor can a Commodore 64\n\ndrive read a file\n\nyou have created on an\n\nIBM Personal Computer.\n\nboth Kaypro and Osborne computers use\n\nthe CP/M operating system\n\nbut can’t read each other’s\n\nhandwriting\n\nfor they format (write\n\non) discs in different\n\nways.\n\nthe Tandy 2000 runs MS-DOS but\n\ncan’t use most programs produced for\n\nthe IBM Personal Computer\n\nunless certain\n\nbits and bytes are\n\naltered\n\nbut the wind still blows over\n\nSavannah\n\nand in the Spring\n\nthe turkey buzzard struts and\n\nflounces before his\n\nhens.\n\n— Charles Bukowski\n\nCharles Bukowski and the Computer\n\nOn Christmas Day, 1990, Charles Bukowski received a Macintosh IIsi computer and a laser printer from his wife, Linda. The computer utilized the 6.0.7 operating system and was installed with the MacWrite II word processing program. By January 18 of the next year, the computer was up and running and so, after a brief period of fumbling and stumbling, was Bukowski. His output of poems doubled in 1991. In letters he remarked that he had more poems than outlets to send them to. The fact that several books of new poems appeared in the years following Bukowski’s death in 1994 can partially be attributed to this amazing burst of creative energy late in life. The Macintosh IIsi helped to enable this creative explosion.\n\nFlying in the face of the adage “You can’t teach an old dog new tricks,” Bukowski kept an open mind about new technologies. Although he wondered if Dostoevsky would have ever used a computer or if he would lose his soul as a writer, Bukowski quickly realized the substantial benefits of the Macintosh and wondered how he ever wrote without one, considering the typewriter archaic. In correspondence, Bukowski championed his computer to friends, stating that they would never regret getting one for themselves. Linda signed Bukowski up for a computer class, and he went willingly, demonstrating his eagerness to master the new technology. A short time later, Bukowski characteristically claimed that he had a secret, foolproof system for dealing with his computer’s many shutdowns and malfunctions, much like he had a system at the racetrack.\n\nIn general Bukowski kept abreast of new innovations that would further his writing. In a letter to John Martin, his Black Sparrow publisher, Bukowski mentioned the availability of a technology (the Internet) that would allow him to send poems instantly. The speed and ease of new technologies amazed, excited, and inspired him. When he first got a fax machine, Bukowski immediately wrote Martin a fax poem. In late 1992, Bruce Kijewski approached Bukowski with the idea of electronic books. Bukowski was intrigued. He wrote back, “Yes, you have a strange project: electronic books. It might be the future as more and more people find that the computer is such a magic thing: time-saver, charmer, energizer.” Bukowski’s open-mindedness in old age is refreshing, when you consider all the aging writers who fall back and rely on the familiar, be it in technologies of writing or actual writing style. But there are still reservations and a sense of nostalgia. The same letter to Kijewski continues, “But, still, when [the electronic book] comes I will still miss the old fashioned book.” Despite such statements, it is clear that Bukowski was a writer not afraid of, or pessimistic about, the future.\n\nBukowski’s embrace of new technology should not surprise me, but it always does. Putting aside the transgressive nature of Bukowski’s subject matter, part of me considers him a conservative poet. On the level of poetics, he rarely impresses me as particularly innovative. For example he never experimented with the page as a field, a technique that I have a weakness for. In addition, his use of poetic form and the line seems rather simple and direct. Yet this was not always so. The poems from the 1960s used a much longer and freer line that incorporated elements of surrealism. There is a playfulness of language in these poems. Bukowski gets drunk on words and the joys of putting them together. This excess gets stripped down later in his career. I tend to see this as a lack of innovation, but it is not; it is an adaptation and, in fact, addresses Bukowski’s intense concern with the line. Bukowski as he got older sought a simpler, more direct poem and used a shorter line. I should think of William Carlos Williams, Louis Zukofsky, or HD when I see Bukowski’s later poems with their four to five word lines. As with new technology, Bukowski possessed an open mind with all manner of writing styles and techniques. He would try anything once. For a brief period in the later 1960s, Bukowski, at the urging of Carl Weissner, addressed the cut-up and flirted with the idea in a few poems. A great letter from the period parodies the cut-up, even though the cut-ups probably came from Bukowski’s imagination rather than the scissors.\n\nI do not want to suggest that Bukowski was pioneer or a radical in his use of the computer. He is no Michael Joyce, to mention a pioneer in the field, and his work never did incorporate the possibilities of, nor test the boundaries of, digital technology, like the more innovative literature of this nature. In fact, Bukowski readily admitted that he used the computer as a typewriter. He marveled at basic capabilities such as formatting, fonts, and spell check. Yet there is more to Bukowski’s relationship with computer than that. In a letter to Ivan Suvanjieff on February 20,, 1992, Bukowski set out in some detail his thoughts on the computer and writing. He writes, “One editor writes me an almost snarling letter. ‘All a computer does is allow you to correct the composition of your work!’ This man understands nothing.” Pound, Olson, and Zukofsky demonstrated the value of the typewriter in composition so this ability on the computer is no small matter, as are fonts and the like, but for Bukowski, the computer actually altered how he felt about and approached his writing. In the same letter he writes, “There is something about seeing your words on a screen before you that makes you send the word with a better bite, sighted in closer to the target. I know a computer can’t make a writer but I think it makes a writer better. Simplicity in writing and simplicity in getting it down, hot and real.” He continues, “When this computer is in the shop and I go back to the electric, it’s like trying to break rock with a hammer. Of course, the essence of writing is there but you have to wait on it, it doesn’t leap from the gut as quickly, you begin to trail your thoughts — your thoughts are ahead of your fingers which are trying to catch up. It causes a block of sorts indeed.” Bukowski directly links the ease of writing on the computer with his later, simpler style. One might think that a computer would lead to a longer line, an increased verbiage, not with Bukowski. The ease of the delete / edit functions was as important as the ability to get one’s thoughts down quickly. In addition, the visual aspect of the screen and the feel of writing on a computer influenced the form of Bukowski’s later poetry.\n\nBukowski also incorporated the computer as a metaphor in his later writing. From early 1991 to his death in 1994, computers and the act of writing on one appeared repeatedly. In The Captain is Out to Lunch and the Sailors Have Taken over the Ship, R. Crumb provided an illustration of Bukowski sitting in front of his Macintosh. The caption reads, “Old writer puts on sweater, sits down, leers into the computer screen and writes about life. How holy can we get?” Clearly, the computer re-energized Bukowski and gave him new life as a writer. Yet much of Bukowski’s late writing was about old age and death. The computer fit into this. In poems, letters, and in The Captain, Bukowski chronicled his struggles with the computer. The shutdowns, the lost poems, the time at the shop for repairs. This mirrored Bukowski’s own health problems and trips to the hospital. The computer represented the writer in old age. The computer and the digital revolution also suggest the end of the book and of print. As a result, the computer spelled the death of the traditional author, a fact that must have struck Bukowski as he faced death himself. Yet all was not doom and gloom as the computer (old age and death) also provides the material and means for new poems. So the computer also represents the old writer’s creative impulse. In the four letters collected in Reach for the Sun for 1994, two mention the computer. In Bukowski’s late writing, the computer simultaneously symbolized the persistent creativity and eventual death of the aging writer.\n\nConsidering the importance of the computer to Bukowski’s later work and creative process, I wonder if his computer, hard drives, and disks were sent to the Huntington Library with the rest of his archive. Again the level of study directed at Bukowski’s computer would not be on the same level as, say, a Michael Joyce, but a familiarity with a Macintosh IIsi, the 6.0.7 operating system, and MacWrite II would provide insight into Bukowski’s working habits and the resulting output. Given the creative charge Bukowski felt facing the black computer screen, a scholar would be well served experiencing the same working environment. Did Bukowski tailor his poems to the screen? Was the journal / diary aspect of The Captain derived from the computer in any way? How did Bukowski edit on the computer? Did his writing process differ from his use of the typewriter or his writing by hand? Bukowski mentioned experimenting with fonts and formatting. Did he keep any of these efforts and what was the extent of this practice? How did he format his blank “page” on the screen?\n\nAs Bukowski’s letters and other writing of the early 1990s show, he was aware of the importance of these questions. Take the poem “16 Bit Intel 8088 Chip.” Here, the incompatibility of computers is contrasted with the harmony of nature. The themes of rebirth, death and the poet in old age are all present as with much of Bukowski’s writing addressing the computer. Yet the poem also touches on a central dilemma for the contemporary librarian and archivist. How do you store and make sense of all the different computer technologies that proliferate in the digital age? The poem also hints at the transitory nature of computer technology. Like microfiche, tape cassettes and CDs, computer operating systems change rapidly and deteriorate. Unlike paper, unlike the traditional, soon to be “obsolete” book. Despite the changing technology, writers and artists will still create, but how is a librarian to keep track of this output and keep it available ten, fifty, a hundred years from now?\n\nAs a generation of baby-boom writers approach the completion of their creative careers, these are becoming central questions for today’s librarians. Currently, the University of Texas Library has 37 author archives that contain computer technology. This number is going to explode in the next decade. Famously, the computers of Ralph Ellison proved a major obstacle in piecing together the thousands of pages of draft of his never-completed follow-up to Invisible Man. Ellison used several different computers with different operating systems to store his drafts.\n\nWhen I was briefly in graduate school, I was required to be fluent in a foreign language. This was a major roadblock for me. Today, a mastery of computers, their operating systems, their languages, and how they work is becoming mandatory for scholars and librarians. They need to be computer programmers and designers as well as experts on bibliographic matters. The questions this technology raises are numerous. How do you create electronic versions of texts? What are the standards and critical approaches to these texts? How are various electronic drafts to be approached and prioritized? What are the ethics of digging into a writer’s personal computer? What is a writer’s obligation to save drafts and email? Such questions are just the tip of the iceberg.\n\nKyle Schlesinger, my co-editor on Mimeo Mimeo, sent me a link to the work of Matthew G. Kirchenbaum, an Associate Professor of English at the University of Maryland. He and other forward-looking academics are addressing these questions. Jerome McGann, as I have mentioned before, is one of the pioneers in this respect. His work on the Rossetti Project and with IVANHOE shows what is possible as well as the endless possibilities. I also mentioned in my Beat Critics article about the desire to create an electronic version of On the Road complete with Kerouac’s revisions and scholarly commentary. As Beat criticism moves away from definitions and canon formation, it will have to address many technological questions. Kirchenbaum and others are mapping this field. The white paper, Approaches to Managing and Collecting Born-Digital Literary Materials for Scholarly Use, is a case in point.\n\nWilliam Burroughs and the Computer\n\nThis takes me to the William Burroughs archive at the Berg. The index of the archive was just posted on the Berg’s website. It makes for fascinating reading. The breadth of the archive is immense and intimidating. Quite possibly, the traditional book may not be the best manner to present the contents of this archive to the public. For example, maybe an electronic Naked Lunch along the lines of the electronic Melville described in John Bryant’s paper on On the Road is what is needed. Thus, making sense of the Burroughs archive may require a solid knowledge of computers and computer design.\n\nThat said, none of the items in the Burroughs archive are born digital, i.e., they were not initially created on a computer or by other digital means. Burroughs used a typewriter or wrote by hand. Everything Lost highlights the incredible set of challenges that results from holograph manuscripts while also showing how modern technology can suggest solutions to these problems. While I have heard mention that Burroughs possessed a computer in Lawrence, I have never seen evidence that he utilized it in his creative process. The Berg shows no computers, disks, hard drives or print outs. This makes sense given that the collection dated from 1951 to 1972, but I am unaware of such items elsewhere, such as Ohio State University. I have never seen an original Burroughs manuscript, such as a laser printout, that shows his work was ever born digital. I believe Last Words was handwritten in a journal. Somebody correct me if I am wrong here.\n\nAs far back as the mid-1960s, Burroughs was aware of the possibilities of the computer and computer-generated poetry. In Insect Trust Gazette, Burroughs’ work appears alongside an early computer poem. In his interview with Conrad Knickerbocker in Paris Review, he stated that he had yet to experiment with the computer, but thought that such literature was valid and interesting, if it stood on its own merit. Yet as time passed — again, as far as I know — Burroughs never experimented with the computer. On one level this makes sense given the fact that Burroughs was well advanced in age and set in his ways by the time the personal computer was generally available. You might say you can’t teach an old dog new tricks, but Bukowski proves that you, in fact, can.\n\nPart of me has always found it weird that Burroughs was not more involved in the Internet and computers. It seems right up his alley. His work has often been connected to cyberpunk fiction as an early influence. His writing is routinely described as a form of hypertext (“You can cut into Naked Lunch at any intersection point”). One might think Burroughs would be curious to explore this aspect of his work further, given the hype surrounding hypertext in the early 1990s. Burroughs readily embraced the technologies of film, tape, and painting into his creative process, so he was open to new media and mediums. The idea of the Composite City and Interzone seem, as critics have noted, to have a particular affinity to hypertext and the Internet. The free-flowing and interconnected nature of the City draws many comparisons to the Web. I tend to link Interzone and the Composite City with Borges’ concept of the Tower of Babel and the all-inclusive library. The idea of an endless novel and a digital archive seem Burroughsian to me. Burroughs, computers, and the Internet seem a match made in, if not heaven, cyberspace.\n\nIn 1994, Wired announced the imminent launch of an official Burroughs website in conjunction with Timothy Leary. It never came off. The venture proved to be largely an example of financial speculation, rather than a creative enterprise. The presence of Leary cements this fact for me. From psychedelics to cyberspace, Leary was more P.T. Barnum than anything else. He was a promoter and a popularizer rather than a true astronaut of inner- or cyberspace. Leary was not a creator. Yet this idea of a website with Burroughs’ input continues to fascinate. What would Burroughs have done with an Ian Sommerville-type collaborator who knew the nuts and bolts of computers and the Internet, was aware of their philosophical and cultural implications, and also possessed a desire to expand the medium creatively? Like many on the forum at RealityStudio, I wonder what if? It was not to be.\n\nMaybe the lack of born-digital material in Burroughs’ archives can be simply explained by the fact that the digital age had passed him by in his old age. An interview from 1987 suggests as much. On the question of Burroughs’ involvement with word processors, he answers, “No, I’m very poor with any mechanical contrivances. I don’t know how a typewriter works, for example. I can use it, but I don’t know how it works. Right now, word processors seem just too complicated to get into. I guess they would be helpful, save a great deal of time, undoubtedly, but at this point the effort involved in learning how to use them just doesn’t seem worthwhile.”\n\nBut there might be more to Burroughs’ less than enthusiastic attitude toward computers than that. By the 1990s, Burroughs had largely left writing behind and was exploring painting more fully. He clearly still burned with the desire to create. The process of painting excited Burroughs, particularly the use of the hand and gesture. In an interview with Klaus Maek in 1990, Burroughs states, “When I started painting, I said, I will have to see with my hands and I just let my hands do it. And my hands, sometimes they know.” The cut-up was a similar physical process. Burroughs needed that direct confrontation with his materials in order to create. He wrote in Naked Lunch: “There is only one thing a writer can write about: what is in front of his senses at the moment of writing.” Painting provided a slightly different intimacy and immediacy. Burroughs states, “For one thing when you are writing you can’t help but know what you’re writing about because it’s right there in front of you, but I never know what I’m painting until I am finished. I sometimes paint with my eyes closed because I see with my hands when I paint. In a sense, painting is easier than writing because you just let your hands do it…” Possibly, the blank computer screen provided too much distance to interest and to inspire Burroughs. As I mentioned Bukowski felt the opposite: the computer got him directly into the writing.\n\nLooking through Burroughs’ archive suggests another roadblock to the computer. The archive clearly demonstrates how intimately Burroughs dealt with the materials of print culture. Burroughs was particularly fascinated by that epitome of mass print culture: the newspaper as well as magazines and journals. Bukowski was interested in the little magazine and underground paper solely as outlets for his work. In contrast, Burroughs sought to detourn mass print culture and turn it back on itself. How mass print culture operated, disseminated, and influenced public opinion intrigued Burroughs. He was also intensely involved with the materiality of print. The printed word was an object to be manipulated. The cut-up and his use of collage in scrapbooks highlight this. Writing on a computer lacks this materiality. Of course this is not true as data recovery makes clear. Even a deleted document leaves a trace burned into the hard drive. Yet the immediacy of the typewriter biting into paper is not there, to say nothing of the pleasure of the act of handwriting. Cutting and pasting digitally lacks the obvious physical effort of scissors and glue. We come back to Burroughs’ pleasure in the tactile.\n\nNostalgia plays a large role in Burroughs’ work. For all of Burroughs’ claims of embracing the future, moving towards the space age (“We are here to go!”), he felt strongly the pull of the past. He looked back fondly at silent films of the 1920s and the same holds true for the print culture of bygone days like boy’s weeklies. The digital age supposedly spells the death of print. Clearly the newspaper as Burroughs knew it is, if not dying, in a profound period of change. Burroughs would have felt that loss keenly. The age of the electronic book and the Internet may have been a world that Burroughs predicted but it seems, looking at his archive and his interest in painting, to have been a world he chose not to get involved in and maybe, creatively, could not embrace. For the most part, Burroughs approached the digital age with what he ultimately sought and demanded in his writing, silence."
  },
  {
    "source": {
      "id": "the-verge",
      "name": "The Verge"
    },
    "author": "Lauren Feiner",
    "title": "Eddy Cue is fighting to save Apple’s $20 billion paycheck from Google",
    "description": "Microsoft's Bing or DuckDuckGo probably won't disrupt Google's dominance in search, said Apple senior vice president of services Eddy Cue - but AI services easily could. Cue was returning to a courtroom in Washington, DC where he last testified in the Justice…",
    "url": "https://www.theverge.com/policy/662974/google-search-remedies-trial-eddy-cue-apple-deal-ai",
    "urlToImage": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/03/236780_Google_AntiTrust_Trial_Custom_Art_CVirginia__0000_4.png?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "publishedAt": "2025-05-07T21:40:34Z",
    "content": "AI will disrupt Googles search monopoly even without a breakup, the Apple exec says.\r\nAI will disrupt Googles search monopoly even without a breakup, the Apple exec says.\r\nMicrosofts Bing or DuckDuck… [+5172 chars]",
    "full_content": "is a senior policy reporter at The Verge, covering the intersection of Silicon Valley and Capitol Hill. She spent 5 years covering tech policy at CNBC, writing about antitrust, privacy, and content moderation reform.\n\nMicrosoft’s Bing or DuckDuckGo probably won’t disrupt Google’s dominance in search, said Apple senior vice president of services Eddy Cue — but AI services easily could.\n\nCue was returning to a courtroom in Washington, DC where he last testified in the Justice Department’s trial against Google’s search monopoly in September 2023. During the current remedies trial on Wednesday, Cue said that in the time since, well-funded generative AI upstarts have made such significant advancements that they could ultimately disrupt that monopoly — perhaps more effectively than the court could.\n\nCue was also, however, there to defend a significant source of Apple’s revenue: the payments Google offers for default search engine placement on Apple’s Safari browser.\n\nAfter the 2023 trial, Judge Amit Mehta found that Google illegally monopolized the online search market, in part through agreements like the Safari deal. For the last two and a half weeks, Google and the DOJ have pitched Mehta on what he should (or shouldn’t) do to fix the problem. The DOJ says Google should be forced to share valuable search data with competitors and spin off its Chrome browser, while Google (which plans to appeal the earlier ruling) wants only restrictions on the deals Mehta deemed unfairly exclusionary. And rather than barring its Apple deal altogether, it would make Google give Apple more flexibility to sign additional default search agreements across its various devices or in private browsing mode.\n\nCue argued Wednesday that rapid AI advancements mean the antitrust threat Mehta identified is shrinking. For the first time in 22 years, Cue said, Apple saw search volume decline in its Safari browser last month — a side effect of users seeking more information from AI chatbots. The DOJ, unsurprisingly, disagrees. It’s not uncommon for technological development to outpace the slow trudge of the court system, but the government says that pace isn’t fast enough to fix a persistent market issue.\n\nApple has a lot of skin in the game here — the DOJ previously revealed it rakes in $20 billion in payments from Google annually. Google’s proposed remedies could reduce it, but they would also open up Apple’s options and preserve much of its revenue flow. The DOJ’s, meanwhile, could wipe out that cash flow altogether. Cue seemed bewildered that Apple could get the short end of the stick for a punishment supposedly inflicted on Google. The idea that the court could decide Google did something wrong and then let it save money at Apple’s expense, he said, “just seems crazy to me.”\n\nGoogle could even keep its prime Safari placement without a deal in the short term, Cue said. “We don’t really have a choice today,” he testified. Even if Apple could cut a new default deal with another company, Google is still the best service available for customers, so they’d probably seek it out — and Apple wouldn’t get a cut. Cue said he’s “lost a lot of sleep” over the prospect of losing the Google deal, and warned it would impact Apple’s ability to make new products.\n\nCue also told Mehta he “can’t say I disagree” with his analysis that Apple was disincentivized from building its own search engine because of its deal with Google. But, he added, “we can’t do everything,” and since Google is really good at what it does, Apple prefers to focus on areas where it can offer unique value.\n\nThere’s “much greater potential because there are new entrants that are attacking the problem in a different way”\n\nAI could eventually change all of this, Cue testified. Apple is already exploring adding AI search options, though it recognizes they can’t yet replace traditional search engines. “To date, they’re just not good enough,” he said.\n\nCue said “good enough” could come sooner than he anticipated. He said there’s “much greater potential because there are new entrants that are attacking the problem in a different way.” Large language model (LLM) AI companies haven’t built a robust enough search index to substitute for Google yet, he said, but combining an LLM with search could let them use a smaller index effectively soon.\n\nEven though he is backing Google’s fight to keep its search deals, he seemed to acknowledge that the DOJ’s proposal to syndicate Google’s search index could get AI rivals up to speed even faster.\n\nCue cautioned the judge that tech is not like other industries, and giants often fall even without court intervention. “When I got to Silicon Valley, all of the best companies, or the most successful companies, either don’t exist today or are significantly smaller and less impactful,” Cue said, pointing to companies like HP, Sun Microsystems, and Intel. In the technology field, being an incumbent might not offer the same benefits it does in other markets. “We’re not an oil company, we’re not toothpaste. These are things that are going to last forever,” he said. “You may not need an iPhone 10 years from now.” After nearly four decades at Apple, Cue now sees AI as “a huge technological shift.”\n\nPutting himself in the judge’s shoes, Cue told Mehta, “we’re lucky, because honestly, if AI had not come about, I don’t know what you could do.” That’s because, he explained, “until there are truly competitive products, people will keep using the best one.”"
  },
  {
    "source": {
      "id": null,
      "name": "Righto.com"
    },
    "author": null,
    "title": "Reverse engineering the 386 processor's prefetch queue circuitry",
    "description": "In 1985, Intel introduced the groundbreaking 386 processor, the first 32-bit processor in the x86 architecture. To improve performance, the ...",
    "url": "http://www.righto.com/2025/05/386-prefetch-circuitry-reverse-engineered.html",
    "urlToImage": "https://lh3.googleusercontent.com/blogger_img_proxy/AEn0k_v_qrqCUi-Alwlt09K9bCKQxp0dp6qK--JOfMpO1ehEptbCRLbMmb-GTOM7GrC2q9wvh4jsxPVMPosDEszzYEr6KxfqBwhb_h9ENucOOTUrbfDBXkWH4jWH0TpKnqY8I9B5SPlFyZ2oig=w1200-h630-p-k-no-nu",
    "publishedAt": "2025-05-10T16:23:06Z",
    "content": "In 1985, Intel introduced the groundbreaking 386 processor, the first 32-bit processor in the x86 architecture.\r\nTo improve performance, the 386 has a 16-byte instruction prefetch queue.\r\nThe purpose… [+26570 chars]",
    "full_content": "Get new posts by email:\n\nSubscribe"
  },
  {
    "source": {
      "id": null,
      "name": "Slashdot.org"
    },
    "author": "msmash",
    "title": "Microsoft Unveils AI-Powered Overhaul for Windows 11",
    "description": "Microsoft has unveiled a substantial AI-focused update for Windows 11 and Copilot+ PCs, introducing features that leverage neural processing units across the operating system. The update centers on AI-powered helpers across core Windows apps, with an intellig…",
    "url": "https://tech.slashdot.org/story/25/05/06/185254/microsoft-unveils-ai-powered-overhaul-for-windows-11",
    "urlToImage": "https://a.fsdn.com/sd/topics/windows_64.png",
    "publishedAt": "2025-05-06T18:05:00Z",
    "content": "Microsoft has unveiled a substantial AI-focused update for Windows 11 and Copilot+ PCs, introducing features that leverage neural processing units across the operating system. The update centers on A… [+1063 chars]",
    "full_content": "Microsoft has unveiled a substantial AI-focused update for Windows 11 and Copilot+ PCs , introducing features that leverage neural processing units across the operating system. The update centers on AI-powered helpers across core Windows apps, with an intelligent agent in Settings that can locate and adjust options via natural voice commands. Key additions include expanded Click To Do functionality, allowing users to draft Word content based on screen context, engage Reading Coach, or send details directly to Excel tables.The Photos app gains a relight feature with support for three customizable light sources, while Paint adds object selection and text-to-sticker generation. Snipping Tool will automatically detect and crop prominent screen content, adding text extraction and color picking capabilities. System-level enhancements include an updated Start menu with phone companion integration, AI-powered actions in File Explorer for content summarization, and text generation in Notepad with new formatting options.Most features will debut first on Windows Insider builds for Snapdragon-powered Copilot+ PCs before expanding to systems with AMD or Intel chips. Several tools, including Ask Copilot and Reading Coach, are already available to Insiders."
  },
  {
    "source": {
      "id": null,
      "name": "Windows Central"
    },
    "author": "sean.endicott@futurenet.com (Sean Endicott)",
    "title": "Qualcomm targets Intel with three new ads for Snapdragon X PCs — \"Here's a little intel on what's really inside\"",
    "description": "Snapdragon X chips are front and center in Qualcomm’s new ads, directly challenging Intel’s dominance in the Windows PC market.",
    "url": "https://www.windowscentral.com/hardware/cpu-gpu-components/qualcomm-targets-intel-with-three-new-ads-for-snapdragon-x-pcs-heres-a-little-intel-on-whats-really-inside",
    "urlToImage": "https://cdn.mos.cms.futurecdn.net/4CAe6JbCQYh5epxgmatvD7.jpg",
    "publishedAt": "2025-05-07T16:53:54Z",
    "content": "Move over Mac vs PC, a new battle is taking center stage. A series of ads from Qualcomm places the company's Snapdragon X processors squarely against the best from Intel.\r\n\"Here's a little intel on w… [+2996 chars]",
    "full_content": "Move over Mac vs PC, a new battle is taking center stage. A series of ads from Qualcomm places the company's Snapdragon X processors squarely against the best from Intel.\n\n\"Here's a little intel on what's really inside,\" jabs one of the ads comparing a PC powered by Snapdragon to one powered by an \"unnamed\" CPU (that's clearly Intel).\n\nOne ad from Qualcomm focuses on figures, such as the fact that Snapdragon X chips run \"at max performance\" while Intel chips drop off in performance when unplugged.\n\nThat claim has merit, though some context is needed. While there are some PCs powered by Intel chips that drop performance to \"as little as 55%,\" it's common to see much smaller dips.\n\nThat same ad highlights the long battery life of PCs powered by Snapdragon as well.\n\nAnother ad is more direct in highlighting the 55% performance claim, showing spouses, soldiers, and CEOs only committing 55% effort.\n\nThe final ad drives home the same point, showing how PCs running at 55% performance when unplugged would affect office producitivy.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nWhat's the Intel - YouTube Watch On\n\nThe Max Performance you deserve - Snapdragon X Series - YouTube Watch On\n\nOffice Tripped – Snapdragon X Series - YouTube Watch On\n\nBest Windows laptops: Is Snapdragon making a push?\n\nMicrosoft's Surface Pro 12-inch and Surface Laptop 13-inch run on Snapdragon X Plus processors. (Image credit: Microsoft)\n\nPeople have dreamed of powerful and efficient computing for years, and many assumed ARM64 architecture would make those types of devices possible. The dream became a reality with the Snapdragon X series of chips.\n\nThe Snapdragon X Elite blew previous ARM64 chips away. The Snapdragon X Plus also delivered a nice midrange option that was also far beyond previous offerings.\n\nOur Senior Editor Zac Bowden said the following about the Snapdragon X Elite in our ASUS Vivobook S 15 review:\n\n\"So, does the Snapdragon X Elite live up to the hype? In short, it absolutely does. This chip is a beast, outputting incredible performance that you can feel in almost every task. Whether browsing the web with lots of tabs, multitasking through lots of open apps, rendering video and audio, hosting a podcast, or even some gaming, the Snapdragon X Elite can do it all.\"\n\nFor a while, PC manufacturers were hesitant to fully embrace PCs powered by ARM64 processors. But things have changed.\n\nThe list of the best Windows on Arm laptops now includes devices from Microsoft, Samsung, HP, and ASUS. And of course, those PCs are all powered by Snapdragon chips (though we could see ARM64 CPUs from NVIDIA at some point).\n\nConsumer-focused Surface devices, including the newly announced Surface Pro 12-inch and Surface Laptop 13-inch run on Snapdragon chips. Microsoft's flagship computers, the Surface Pro 11 and Surface Laptop 7 also have Snapdragon X chips inside.\n\nThere are versions of the Surface Pro 11 and Surface Laptop 7 with Intel chips, but those PCs are built for business users.\n\nMicrosoft has put extensive effort into optimizing Windows 11 for PCs powered by Snapdragon chips. Developers and major corporations have also optimized programs, greatly expanding the library of the best native Windows on Arm apps.\n\nLooping back to the ads from Qualcomm, that company's Snapdragon chips are the star of the show here. Any efforts from Microsoft would fall short without chips like the Snapdragon X Elite.\n\nNow that Windows PCs powered by Snapdragon chips are in the spotlight, Qualcomm seems eager to show its processors can go toe-to-toe with Intel CPUs."
  },
  {
    "source": {
      "id": null,
      "name": "Yahoo Entertainment"
    },
    "author": "Reuters",
    "title": "Intel shareholders approve equity incentive plan, new CEO pay",
    "description": "SAN FRANCISCO (Reuters) -Intel shareholders on Tuesday approved a measure by the chipmaker aimed at topping up share reserves to attract and retain new...",
    "url": "https://finance.yahoo.com/news/intel-shareholders-approve-equity-incentive-164635550.html",
    "urlToImage": "https://media.zenfs.com/en/reuters-finance.com/5a81854c26f6f75fc8f9651c05941cc4",
    "publishedAt": "2025-05-06T16:46:35Z",
    "content": "SAN FRANCISCO (Reuters) -Intel shareholders on Tuesday approved a measure by the chipmaker aimed at topping up share reserves to attract and retain new employees, and compensation for its new CEO Lip… [+1263 chars]",
    "full_content": "SAN FRANCISCO (Reuters) -Intel shareholders on Tuesday approved a measure by the chipmaker aimed at topping up share reserves to attract and retain new employees, and compensation for its new CEO Lip-Bu Tan.\n\nIntel shares fell 1.6% in early afternoon trading after dropping 36% in the past year.\n\nShareholders approved the Santa Clara, California-based company's board of directors, though three members did not stand for reelection. Tan, who took the helm in March after the board ousted former CEO Pat Gelsinger in December, will get stock awards of $42 million depending on how Intel's shares perform.\n\nThree shareholder proposals were rejected. The proposals would have required Intel to reassess its operations in Israel, produce new reports on charitable giving and give shareholders the right to act by written consent.\n\nThe shareholders meeting was the first for Tan. The board had lost confidence in Gelsinger's costly turnaround plan after he failed to deliver on lofty promises.\n\nTan has begun to restructure Intel by flattening the leadership hierarchy and articulating a plan to rebuild its artificial intelligence business and cut its large middle management layer.\n\nTan said he intends to leverage Intel's large share in the personal computer and data center markets to deliver more competitive products, and will refine its AI strategy.\n\n(Reporting by Stephen Nellis and Max A. Cherney in San Francisco; Editing by Franklin Paul and Richard Chang)"
  },
  {
    "source": {
      "id": "business-insider",
      "name": "Business Insider"
    },
    "author": "Ben Bergman",
    "title": "Nvidia-backed Israeli AI startup AI21 is raising a $300 million funding round",
    "description": "AI21, an Israeli startup building its own large language models (LLMs), is raising a $300 million funding round, according to a source.",
    "url": "https://www.businessinsider.com/llm-startup-ai21-is-raising-a-300-million-funding-round-2025-5",
    "urlToImage": "https://i.insider.com/681d09fac6ad288d14806e1d?width=1200&format=jpeg",
    "publishedAt": "2025-05-09T09:00:02Z",
    "content": "AI21's founders from left to right: Ori Goshen, Amnon Shashua and Yoav Shoham.Roei Shor\r\n<ul><li>AI21 is raising a $300 million Series D funding round to build its own LLMs.</li><li>AI21 aims to redu… [+1904 chars]",
    "full_content": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nAI21 Labs, an Israeli startup building its own large language models, is raising a $300 million Series D funding round, a person with knowledge of the deal said.\n\nThe valuation could not be learned. The company last raised $208 million at a $1.4 billion valuation in 2023. This round would bring the company's total funding to $636 million.\n\nAI21 was founded in 2017 by the entrepreneurs and AI researchers Amnon Shashua (the founder and CEO of Mobileye), Yoav Shoham (a professor emeritus at Stanford University and a former principal scientist at Google), and Ori Goshen (a serial entrepreneur and the founder of CrowdX).\n\nWhile many AI startups rely on LLMs built by companies like Anthropic or OpenAI, AI21 is building its LLMs from the ground up.\n\nIts goal is to make generative AI more dependable for companies by reducing hallucinations, which are when LLMs present false information as fact.\n\nIn March, AI21 launched an AI orchestration system called Maestro, which it says can reduce hallucinations by 50% and boost reasoning model accuracy to over 95%.\n\nExisting investors in AI21 include Google, Nvidia, Intel Capital, Walden Catalyst, Pitango, SCB 10X, B2venture, Samsung Next, Comcast Ventures, and Ahren Innovation Capital. Customers include Fnac, Capgemini, and the website builder Wix.\n\nAbout $7 billion in investment went to AI-related startups last month, representing 30% of venture funding worldwide, according to Crunchbase data.\n\nIsraeli tech companies have raised more than $12 billion in 2024, up 31% from 2023, Startup Nation Central says. The Israeli cybersecurity company Wiz was just acquired by Google for $32 billion, the search giant's largest-ever acquisition."
  },
  {
    "source": {
      "id": "the-next-web",
      "name": "The Next Web"
    },
    "author": "Siôn Geschwindt",
    "title": "German dual-use drone maker becomes unicorn amid defence tech boom",
    "description": "German drone maker Quantum Systems has raised €160mn at a valuation north of €1bn, becoming Europe’s latest defence tech unicorn.   Quantum Systems builds electric, AI-powered autonomous surveillance drones that are dual-use, meaning they can serve both milit…",
    "url": "https://thenextweb.com/news/german-dual-use-drone-maker-becomes-unicorn-defence-tech-boom",
    "urlToImage": "https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2025%2F05%2FVector-AI-x2-quantum-systems-drone-startup-1.jpg&signature=f7afb1510cd682b37fc3e1b619a756cc",
    "publishedAt": "2025-05-06T13:32:24Z",
    "content": "German drone maker Quantum Systems has raised 160mn at a valuation north of 1bn, becoming Europes latest defence tech unicorn.  \r\nQuantum Systems builds electric, AI-powered autonomous surveillance d… [+2706 chars]",
    "full_content": "German drone maker Quantum Systems has raised €160mn at a valuation north of €1bn, becoming Europe’s latest defence tech unicorn.\n\nQuantum Systems builds electric, AI-powered autonomous surveillance drones that are dual-use, meaning they can serve both military and civil purposes.\n\nDefence forces can use the unmanned aerial vehicles (UAVs) to spy on enemies and gather intel. The drones can also be used by farmers to inspect their crops, by energy companies to check power lines, and by search and rescue teams to look for survivors.\n\nFlorian Seibel, co-CEO and co-founder of Quantum Systems, said the company was ready to become the European leader in “robotised and AI-powered aerial intelligence solutions” for both defence and civilian applications. “The need for sovereign, aerial intelligence has never been more pressing,” he added.\n\nThe 💜 of EU tech The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now!\n\nThe latest funding round, led by the UK’s Balderton Capital, brings Quantum Systems’ total capital raised to $284mn (€250mn), according to Dealroom data. The round also saw participation from Airbus Defence and Space, Bullhound Capital, and existing angel investor Peter Thiel.\n\nQuantum Systems has already secured several major defence contracts, including with the German, Spanish, and British armed forces. Its flagship drone, the Vector AI, has been tested in Ukraine, which has become a major battlefield testing ground for dual-use technologies.\n\nTekever is another European startup that has surveillance drones in active deployment in Ukraine. Founded in Portugal, the dual-use firm recently raised “tens of millions” in fresh capital at a valuation “beyond $1bn (€882mn),” its CEO, Ricardo Mendes, told the Financial Times.\n\nSimilarly to Quantum Systems, soldiers use Tekever’s long-distance UAVs to cover vast areas and carry out reconnaissance. Its drones have also been used to search for small-boat migrant smugglers in the English Channel and monitor pipelines in Nigeria.\n\nQuantum Systems and Tekever are both benefiting from a surge in European defence tech investment. This growth is partly fueled by rising security concerns — driven by Russia’s aggression and the Trump administration’s warning that it may cut US military support to Europe.\n\nDefence tech startups in Europe raised $613mn (€540mn) in venture capital in 2024, the largest value ever recorded, according to Dealroom. The bulk of those funds was secured by just one company — Germany’s Helsing.\n\nDefence tech is a key theme of the Assembly, the invite-only policy track of TNW Conference. The event takes place in Amsterdam on June 19 — a week before the NATO Summit arrives in the city.\n\nTickets for TNW Conference are now on sale — use the code TNWXMEDIA2025 at the checkout to grab an exclusive discount."
  },
  {
    "source": {
      "id": null,
      "name": "Gizmodo.com"
    },
    "author": "Kyle Barr",
    "title": "I Desperately Hope This Lenovo Gaming Laptop Can Make 3D Screens Mainstream",
    "description": "Glasses-free 3D gaming refuses to die.",
    "url": "https://gizmodo.com/i-desperately-hope-this-lenovo-gaming-laptop-can-make-3d-screens-mainstream-2000599809",
    "urlToImage": "https://gizmodo.com/app/uploads/2025/05/lenovo9i.jpg",
    "publishedAt": "2025-05-08T18:45:26Z",
    "content": "Lenovo is one of the few big hardware brands that seems to think the next big thing in laptopsbeyond better processors and even more glowy RGB lightsis glasses-free 3D displays. The Lenovo Legion 9i … [+3644 chars]",
    "full_content": "Lenovo is one of the few big hardware brands that seems to think “the next big thing” in laptops—beyond better processors and even more glowy RGB lights—is glasses-free 3D displays. The Lenovo Legion 9i should support 30 games with 3D effects, though it may be brought down by an enormous, tariff-boosted price tag. Ignoring the possible enormous price, I’m left holding out hope that lenticular lenses can find a niche in today’s gaming environment.\n\nThe Legion 9i is one of those kitchen sink, desktop replacement laptops meant for both gamers and creatives who want to do everything from one device, even one that weighs nearly eight pounds. That’s nothing new, though what’s different about this device is its 18-inch “PureSight” display that supports 4K resolution in 2D alongside 2K (1,920 x 1,200) resolution in 3D. Like what we experienced with the Samsung Odyssey 3D gaming monitor and Lenovo’s concept 3D curved displays, the screen is an IPS LCD panel built with a lenticular lens array. This creates a kind of pseudo 3D popout effect along with a subtle hint of depth on images. If you ever tried the 3D effect on a Nintendo 3DS, then you have a good idea what this looks like.\n\nThe Nintendo 3DS was also notorious for requiring pitch-perfect viewing angles for the 3D effect. The technology has improved thanks to the use of eye-tracking cameras that beam the image to each eye individually. Lenovo says users will be able to control this using the Lenovo 3D Studio software. The company claims this supports 3D viewing in “a myriad of video, image, and streaming formats” including some creation apps.\n\nYes, the 3D effect is a novelty for a niche crowd of gamers, but from what I played, the effect enhances the otherworldliness of games with more stylized visuals. Our main gripe with the Samsung Odyssey 3D was its mere 13 supported games when the monitor itself cost $2,000. Lenovo told Gizmodo this laptop supports 30 games, including major titles of the last several years like Cyberpunk 2077, Death Stranding, Black Myth: Wukong, Forza Horizon 5, and Fallout 4. We have yet to see this 3D effect when playing a first-person game, and we’d be concerned the view would lead to a sense of queasiness when a cyberpsycho’s monofilament blade aimed at your character pops out of the screen like a knife aimed at your eyes.\n\nThe screen also supports variable refresh rate (VRR) that can switch from 240Hz at 4K to a blistering 440Hz at FHD. That’s handy, since the laptop is set to contain an Intel Core 9 275HX CPU, an Nvidia GeForce RTX 5090 laptop GPU, plus up to 192GB of RAM. We still don’t have pricing, but considering the stated specs, it could be an enormously expensive device. The Legion 9i could cost even more than the MSI Titan 18 HX, one of the beefiest and most expensive laptops I’ve ever reviewed. At least it supports four slots apiece for RAM and SSD storage extensions.\n\nEven the cover of the laptop is unique, sporting a camo-like pattern made of eight layers of carbon fiber. The gaming laptop won’t be available until fall this year, although some gamers and creators in China will likely be the first to check it out. Trump tariffs have pushed prices up for most gadgets and many other products besides, so we aren’t holding out much hope for a product most people can actually afford.\n\nEven with 30 games on offer at launch, that still may not be enough to push 3D screens into the mainstream. We asked Lenovo if the Legion 9i could support third-party 3D software, like Samsung’s Reality Hub used on its Odyssey 3D, but the company declined to say. The cost of concept devices like this will only get steeper as time goes on."
  },
  {
    "source": {
      "id": null,
      "name": "Yahoo Entertainment"
    },
    "author": null,
    "title": "Intel Arrow Lake processors bottleneck PCIe 5.0 NVMe SSDs by 16%, limiting peak speeds to 12GB/s instead of 14GB/s",
    "description": null,
    "url": "https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_61325e9a-b38f-4195-92da-c8ee1f0147e2",
    "urlToImage": null,
    "publishedAt": "2025-05-08T11:23:12Z",
    "content": "If you click 'Accept all', we and our partners, including 241 who are part of the IAB Transparency &amp; Consent Framework, will also store and/or access information on a device (in other words, use … [+714 chars]",
    "full_content": null
  }
]