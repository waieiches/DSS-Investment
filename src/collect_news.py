import os
import json
import requests
from datetime import datetime, timedelta
from dotenv import load_dotenv
import click
from newspaper import Article

# .envì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
NEWS_API_KEY = os.getenv("NEWS_API_KEY")

def extract_full_content(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return None

# ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
def collect_news(ticker, name, days=7, page_size=20):
    url = "https://newsapi.org/v2/everything"
    from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
    to_date = datetime.now().strftime('%Y-%m-%d')

    params = {
        "q": f"{name} OR {ticker}",
        "from": from_date,
        "to": to_date,
        "language": "en",
        "sortBy": "relevancy",
        "pageSize": page_size,
        "apiKey": NEWS_API_KEY
    }

    response = requests.get(url, params=params)
    if response.status_code != 200:
        raise Exception(f"API ì—ëŸ¬: {response.status_code} - {response.text}")

    articles = response.json().get("articles", [])
    print(f"âœ… {ticker} ê¸°ì‚¬ ìˆ˜: {len(articles)}")
    
    for article in articles:
        url = article.get("url")
        if url:
            full_content = extract_full_content(url)
            article["full_content"] = full_content
        else:
            article["full_content"] = None
    return articles

# ì €ì¥ í•¨ìˆ˜
def save_news(ticker, articles):
    date_str = datetime.now().strftime('%Y-%m-%d')
    os.makedirs("data/news", exist_ok=True)
    path = f"data/news/{ticker}_{date_str}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(articles, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {path}")

# CLI ì‹¤í–‰ ëª¨ë“œ
@click.command()
@click.option('--ticker', prompt='ê¸°ì—… í‹°ì»¤', help='ì˜ˆ: TSLA')
@click.option('--name', prompt='ê¸°ì—… ì´ë¦„', help='ì˜ˆ: Tesla')
@click.option('--days', default=7, help='ë©°ì¹  ì „ë¶€í„° ìˆ˜ì§‘í• ì§€')
@click.option('--page_size', default=20, help='ë‰´ìŠ¤ ê°œìˆ˜')
def cli_mode(ticker, name, days, page_size):
    print(f"ğŸ” {ticker} ({name}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    articles = collect_news(ticker, name, days, page_size)
    save_news(ticker, articles)

# ìë™ ìˆœíšŒ ìˆ˜ì§‘ ëª¨ë“œ
def collect_all_from_json():
    with open("data/companies.json", "r") as f:
        all_companies = json.load(f)

    for theme, companies in all_companies.items():
        print(f"\nğŸ”· [í…Œë§ˆ: {theme}]")
        for company in companies:
            ticker = company.get("ticker")
            name = company.get("name")
            if not ticker or not name:
                continue
            try:
                articles = collect_news(ticker, name)
                save_news(ticker, articles)
            except Exception as e:
                print(f"âŒ {ticker} ì‹¤íŒ¨: {e}")

if __name__ == '__main__':
    import sys
    if '--ticker' in sys.argv:
        cli_mode()  # ëª…ë ¹ì–´ ê¸°ë°˜ ì‹¤í–‰
    else:
        collect_all_from_json()  # ì „ì²´ ê¸°ì—… ìë™ ìˆ˜ì§‘

# python src/collect_news.py (--ticker TSLA --name Tesla --days 5)
# --ticker â†’ ì¢…ëª©ì½”ë“œ
# --name â†’ ê²€ìƒ‰ì— ì“¸ ê¸°ì—… ì´ë¦„
# --days â†’ ìµœê·¼ Nì¼ ê°„ì˜ ë‰´ìŠ¤
# --page_size â†’ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜

# collect_news(ticker, name) â†’ ê°œë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
# save_news(ticker, articles) â†’ JSON ì €ì¥
# cli_mode() â†’ CLI ëª…ë ¹ì–´ë¡œ ë‹¨ì¼ ê¸°ì—… ìˆ˜ì§‘
# collect_all_from_json() â†’ companies.json ìë™ ìˆœíšŒ ìˆ˜ì§‘
# --tickerê°€ ìˆìœ¼ë©´ CLI ì‹¤í–‰, ì—†ìœ¼ë©´ ìë™ ì „ì²´ ìˆ˜ì§‘


